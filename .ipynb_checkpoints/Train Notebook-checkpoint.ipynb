{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02a99d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeppy_project as dp\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "LearnFrame = dp.LearnFrame\n",
    "SANE = dp.models.cv.Sane\n",
    "IngpDataset = dp.dataset.IngpDataset\n",
    "UniquePerBatchSampler = dp.sampler.UniquePerBatchSampler\n",
    "DatasetLoader = dp.DatasetLoader\n",
    "\n",
    "\n",
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree('logs')\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce702cff",
   "metadata": {},
   "source": [
    "# Configure Hyper Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8490c934",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sequence Length\n",
    "pos_token_size = 10\n",
    "mlp_token_size = 53\n",
    "hash_token_size = 380 - pos_token_size - mlp_token_size\n",
    "window_size = pos_token_size + mlp_token_size + hash_token_size\n",
    "\n",
    "#Training Parameters\n",
    "batch_size = 32\n",
    "gradient_accumulation_steps = 1\n",
    "lr = 2e-4\n",
    "total_epocs = 20\n",
    "gamma = np.asarray([0.4, 0.05, 0.53, 0.02])\n",
    "gamma = gamma / np.sum(gamma)\n",
    "\n",
    "#Transformer Parameters\n",
    "input_dim = 256\n",
    "embed_dim = 256 * 4\n",
    "latent_dim = 128\n",
    "num_heads = 4\n",
    "num_layers = 4\n",
    "dropout = 0.3\n",
    "bias = False\n",
    "projection_dim = 128\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54601963",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_config = {\n",
    "    'hash_encoding': {\n",
    "        'num_levels': 16,\n",
    "        'level_dim': 2,\n",
    "        'input_dim': 3,\n",
    "        'log2_hashmap_size': 19,\n",
    "        'base_resolution': 16\n",
    "    },\n",
    "    'mlp': {\n",
    "        'num_layers': 3,  # Number of layers in geometric MLP\n",
    "        'hidden_dim': 64,  # Hidden dimension size\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f0e897",
   "metadata": {},
   "source": [
    "# Initialize CUDA environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d23e0d14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "logs\n",
      "checkpoints\n"
     ]
    }
   ],
   "source": [
    "dp.env_config.use_amp = True\n",
    "dp.env_config.torch_compile = True\n",
    "print(dp.env_config.device)\n",
    "print(dp.env_config.log_dir)\n",
    "print(dp.env_config.checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc89fe7",
   "metadata": {},
   "source": [
    "# Configure DatasetLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1949b25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train dataset size : 302\n",
      "train dataloader size : 180000\n",
      "test dataset size : 41\n",
      "test dataset size : 20000\n"
     ]
    }
   ],
   "source": [
    "#Configure IngpDataset object\n",
    "#Similar to torch dataset\n",
    "#__len__\n",
    "#__getitem__(idx)\n",
    "dataset_config = {\n",
    "    \"data_path\": \"../data/\",\n",
    "    \"config\": data_config,\n",
    "    \"window_size\": window_size,\n",
    "    \"token_size\": input_dim,\n",
    "    \"pos_token_size\" : pos_token_size,\n",
    "    \"mlp_token_size\" : mlp_token_size,\n",
    "    \"permutation_augment\" : True,\n",
    "    \"data_buffer_size\" : 1\n",
    "}\n",
    "\n",
    "dataset = IngpDataset(**dataset_config)\n",
    "\n",
    "#Create torch dataloader arguments\n",
    "#Shuffle = False because of UniquePerBatchSampler\n",
    "#The sampler will shuffle the indices\n",
    "dataloader_args = {\n",
    "    \"num_workers\" : 24,\n",
    "    \"prefetch_factor\" : 3,\n",
    "    \"shuffle\" : False,\n",
    "    \"pin_memory\" : True,\n",
    "    \"persistent_workers\" : True,\n",
    "    \"drop_last\" : True\n",
    "}\n",
    "\n",
    "#Create deeppy datasetloader object\n",
    "#Automatically create test-valid splits\n",
    "#Automatically handles dataloaders\n",
    "#UniquePerBatchSampler\n",
    "     # it increase len(dataset) -> len(dataset) * repeat\n",
    "     # it still sample indices without replacement untill the dataset is exhausted\n",
    "     # it guarantees that only one instance of an object is in the batch, so different views of the same object\n",
    "            #is not compared to itself as a negative sample\n",
    "datasetloader_args = {\n",
    "    \"data\" : dataset,\n",
    "    \"batch_size\" : batch_size,\n",
    "    \"splits\" : [0.88, 0.12, 0], \n",
    "    \"dataloader_args\" : dataloader_args,\n",
    "    \"sampler\" : UniquePerBatchSampler,\n",
    "    \"sampler_args\" : {\"num_repeats\" : 20000},\n",
    "}\n",
    "\n",
    "\n",
    "data = DatasetLoader(**datasetloader_args)\n",
    "print(f\"train dataset size : {len(data.train_dataset)}\")\n",
    "print(f\"train dataloader size : {len(data.train_loader)}\")\n",
    "print(f\"test dataset size : {len(data.test_dataset)}\")\n",
    "print(f\"test dataset size : {len(data.test_loader)}\")\n",
    "\n",
    "\n",
    "max_positions = data.train_dataset.dataset.max_positions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ccf5663",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 20 - steps : 100000\n"
     ]
    }
   ],
   "source": [
    "#Calculate total parameter numbers in the dataset to calculate how many steps is an epoch\n",
    "n_object = len(data.train_dataset) * 3 #One object and n augmentation\n",
    "n_param = 7000000\n",
    "t_param = n_object * n_param\n",
    "\n",
    "pass_param = (hash_token_size) * (input_dim / 2) * (batch_size * gradient_accumulation_steps)\n",
    "\n",
    "\n",
    "\n",
    "epochs = int(t_param/pass_param) * total_epocs\n",
    "\n",
    "save_freq = 5000\n",
    "steps = ((epochs + save_freq - 1) // save_freq) * save_freq\n",
    "print(f\"epoch : {total_epocs} - steps : {steps}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7b9a5b",
   "metadata": {},
   "source": [
    "# SANE Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c07d7a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/baris/anaconda3/envs/deeppy/lib/python3.13/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.norm_first was True\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#Create OneCycleLR scheduler (deeppy.nn.optimizer.scheduler)\n",
    "Scheduler_params = {\n",
    "                \"scheduler\" : torch.optim.lr_scheduler.OneCycleLR,\n",
    "                \"auto_step\":True,\n",
    "                 \"max_lr\": lr,\n",
    "                \"total_steps\": steps,\n",
    "                \"pct_start\": 0.20,\n",
    "                \"anneal_strategy\": \"cos\",\n",
    "                \"cycle_momentum\": True,\n",
    "                \"base_momentum\": 0.85,\n",
    "                \"max_momentum\": 0.95,\n",
    "                \"div_factor\": 25,\n",
    "                \"final_div_factor\": 10000.0,\n",
    "                \"three_phase\": False,\n",
    "                \"last_epoch\": -1,\n",
    "}\n",
    "\n",
    "#Create optimizer (deeppy.nn.optimizer.optimizer)\n",
    "Optimizer_params = {\n",
    "    \"optimizer\":torch.optim.AdamW,\n",
    "    \"optimizer_args\":{\"lr\":lr,  \"weight_decay\" : 1e-3, \"fused\" : True, \"amsgrad\":False},\n",
    "    \"clipper\":torch.nn.utils.clip_grad_norm_,\n",
    "    \"clipper_params\":{\"max_norm\" : 5.0},\n",
    "    \"scheduler_params\":Scheduler_params,\n",
    "    \"gradient_accumulation_steps\" : gradient_accumulation_steps\n",
    "}\n",
    "\n",
    "#Create Sane\n",
    "Sane_params = {\n",
    "    \"optimizer_params\":Optimizer_params,\n",
    "    \"max_positions\" : max_positions,\n",
    "    \"input_dim\":input_dim,\n",
    "    \"latent_dim\":latent_dim,\n",
    "    \"projection_dim\" : projection_dim,\n",
    "    \"embed_dim\":embed_dim,\n",
    "    \"num_heads\":num_heads,\n",
    "    \"num_layers\":num_layers,\n",
    "    \"context_size\":window_size,\n",
    "    \"dropout\":dropout,\n",
    "    \"bias\" : bias,\n",
    "    \"gamma\" : gamma,\n",
    "    \"ntx_temp\" : 0.1,\n",
    "    \"pos_token_size\" : pos_token_size,\n",
    "    \"noise_augment\" : 0.1,\n",
    "}\n",
    "\n",
    "\n",
    "model = SANE(**Sane_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95a891c5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autoencoder\n",
      "Network(\n",
      "  (model): Sequential(\n",
      "    (0): SaneLinearTokenizerBeforePosition(\n",
      "      (linear_hash): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      (linear_mlp): Linear(in_features=256, out_features=1024, bias=True)\n",
      "    )\n",
      "    (1): SaneXYZPositionalEmbedding(\n",
      "      (hash_xyz): Linear(in_features=3, out_features=1024, bias=True)\n",
      "      (hash_index_global): ChunkwisePositionalEmbedding(\n",
      "        (positional_embedding): Embedding(6098109, 8)\n",
      "      )\n",
      "      (hash_layer): ChunkwisePositionalEmbedding(\n",
      "        (positional_embedding): Embedding(16, 8)\n",
      "      )\n",
      "      (hash_index_layerwise): ChunkwisePositionalEmbedding(\n",
      "        (positional_embedding): Embedding(700000, 8)\n",
      "      )\n",
      "      (mlp_embed): Embedding(63, 1024)\n",
      "    )\n",
      "    (2): Dropout(p=0.3, inplace=False)\n",
      "    (3): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=False)\n",
      "          )\n",
      "          (linear1): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (dropout): Dropout(p=0.3, inplace=False)\n",
      "          (linear2): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.3, inplace=False)\n",
      "          (dropout2): Dropout(p=0.3, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (4): Linear(in_features=1024, out_features=128, bias=True)\n",
      "    (5): SaneLinearTokenizerBeforePosition(\n",
      "      (linear_hash): Linear(in_features=128, out_features=1024, bias=True)\n",
      "      (linear_mlp): Linear(in_features=128, out_features=1024, bias=True)\n",
      "    )\n",
      "    (6): SaneXYZPositionalEmbedding(\n",
      "      (hash_xyz): Linear(in_features=3, out_features=1024, bias=True)\n",
      "      (hash_index_global): ChunkwisePositionalEmbedding(\n",
      "        (positional_embedding): Embedding(6098109, 8)\n",
      "      )\n",
      "      (hash_layer): ChunkwisePositionalEmbedding(\n",
      "        (positional_embedding): Embedding(16, 8)\n",
      "      )\n",
      "      (hash_index_layerwise): ChunkwisePositionalEmbedding(\n",
      "        (positional_embedding): Embedding(700000, 8)\n",
      "      )\n",
      "      (mlp_embed): Embedding(63, 1024)\n",
      "    )\n",
      "    (7): Dropout(p=0.3, inplace=False)\n",
      "    (8): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=False)\n",
      "          )\n",
      "          (linear1): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "          (dropout): Dropout(p=0.3, inplace=False)\n",
      "          (linear2): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "          (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.3, inplace=False)\n",
      "          (dropout2): Dropout(p=0.3, inplace=False)\n",
      "          (activation): GELU(approximate='none')\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (9): Linear(in_features=1024, out_features=256, bias=True)\n",
      "  )\n",
      "  (encode): OptimizedModule(\n",
      "    (_orig_mod): Sequential(\n",
      "      (0): SaneLinearTokenizerBeforePosition(\n",
      "        (linear_hash): Linear(in_features=256, out_features=1024, bias=True)\n",
      "        (linear_mlp): Linear(in_features=256, out_features=1024, bias=True)\n",
      "      )\n",
      "      (1): SaneXYZPositionalEmbedding(\n",
      "        (hash_xyz): Linear(in_features=3, out_features=1024, bias=True)\n",
      "        (hash_index_global): ChunkwisePositionalEmbedding(\n",
      "          (positional_embedding): Embedding(6098109, 8)\n",
      "        )\n",
      "        (hash_layer): ChunkwisePositionalEmbedding(\n",
      "          (positional_embedding): Embedding(16, 8)\n",
      "        )\n",
      "        (hash_index_layerwise): ChunkwisePositionalEmbedding(\n",
      "          (positional_embedding): Embedding(700000, 8)\n",
      "        )\n",
      "        (mlp_embed): Embedding(63, 1024)\n",
      "      )\n",
      "      (2): Dropout(p=0.3, inplace=False)\n",
      "      (3): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-3): 4 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (linear1): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "            (dropout): Dropout(p=0.3, inplace=False)\n",
      "            (linear2): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.3, inplace=False)\n",
      "            (dropout2): Dropout(p=0.3, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (4): Linear(in_features=1024, out_features=128, bias=True)\n",
      "    )\n",
      "  )\n",
      "  (decode): OptimizedModule(\n",
      "    (_orig_mod): Sequential(\n",
      "      (5): SaneLinearTokenizerBeforePosition(\n",
      "        (linear_hash): Linear(in_features=128, out_features=1024, bias=True)\n",
      "        (linear_mlp): Linear(in_features=128, out_features=1024, bias=True)\n",
      "      )\n",
      "      (6): SaneXYZPositionalEmbedding(\n",
      "        (hash_xyz): Linear(in_features=3, out_features=1024, bias=True)\n",
      "        (hash_index_global): ChunkwisePositionalEmbedding(\n",
      "          (positional_embedding): Embedding(6098109, 8)\n",
      "        )\n",
      "        (hash_layer): ChunkwisePositionalEmbedding(\n",
      "          (positional_embedding): Embedding(16, 8)\n",
      "        )\n",
      "        (hash_index_layerwise): ChunkwisePositionalEmbedding(\n",
      "          (positional_embedding): Embedding(700000, 8)\n",
      "        )\n",
      "        (mlp_embed): Embedding(63, 1024)\n",
      "      )\n",
      "      (7): Dropout(p=0.3, inplace=False)\n",
      "      (8): TransformerEncoder(\n",
      "        (layers): ModuleList(\n",
      "          (0-3): 4 x TransformerEncoderLayer(\n",
      "            (self_attn): MultiheadAttention(\n",
      "              (out_proj): NonDynamicallyQuantizableLinear(in_features=1024, out_features=1024, bias=False)\n",
      "            )\n",
      "            (linear1): Linear(in_features=1024, out_features=4096, bias=False)\n",
      "            (dropout): Dropout(p=0.3, inplace=False)\n",
      "            (linear2): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "            (norm1): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (norm2): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "            (dropout1): Dropout(p=0.3, inplace=False)\n",
      "            (dropout2): Dropout(p=0.3, inplace=False)\n",
      "            (activation): GELU(approximate='none')\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (9): Linear(in_features=1024, out_features=256, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of parameters (as tensors) : 72\n",
      "Number of parameters in optimizer (as tensors) : 72\n"
     ]
    }
   ],
   "source": [
    "print(\"Autoencoder\")\n",
    "net = 0\n",
    "print(model.nets[net])\n",
    "\n",
    "\n",
    "len_parameters = len(list(model.nets[net].parameters()))\n",
    "\n",
    "op_params = []\n",
    "for group in model.optimizer.optimizer.param_groups:\n",
    "    for p in group[\"params\"]:\n",
    "        if f\"Net{net}\" in p.dpname:\n",
    "            op_params.append(p.dpname)\n",
    "print(f\"Number of parameters (as tensors) : {len_parameters}\")\n",
    "print(f\"Number of parameters in optimizer (as tensors) : {len(op_params)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2116b8c7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTX HEAD\n",
      "Network(\n",
      "  (model): OptimizedModule(\n",
      "    (_orig_mod): Sequential(\n",
      "      (0): AttentionPooling()\n",
      "      (1): Linear(in_features=128, out_features=128, bias=True)\n",
      "      (2): ReLU()\n",
      "      (3): Linear(in_features=128, out_features=64, bias=True)\n",
      "      (4): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of parameters (as tensors) : 5\n",
      "Number of parameters in optimizer (as tensors) : 5\n"
     ]
    }
   ],
   "source": [
    "print(\"NTX HEAD\")\n",
    "net = 1\n",
    "print(model.nets[net])\n",
    "\n",
    "\n",
    "len_parameters = len(list(model.nets[net].parameters()))\n",
    "\n",
    "op_params = []\n",
    "for group in model.optimizer.optimizer.param_groups:\n",
    "    for p in group[\"params\"]:\n",
    "        if f\"Net{net}\" in p.dpname:\n",
    "            op_params.append(p.dpname)\n",
    "print(f\"Number of parameters (as tensors) : {len_parameters}\")\n",
    "print(f\"Number of parameters in optimizer (as tensors) : {len(op_params)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "da17cbeb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotation HEAD\n",
      "Network(\n",
      "  (model): OptimizedModule(\n",
      "    (_orig_mod): Sequential(\n",
      "      (0): concatInputsWithPosition(\n",
      "        (unique_pos): Embedding(10, 128)\n",
      "        (layer_pos): Embedding(2, 128)\n",
      "        (rot_token): Embedding(1, 128)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=128, out_features=256, bias=True)\n",
      "        (dropout): Dropout(p=0.3, inplace=False)\n",
      "        (linear2): Linear(in_features=256, out_features=128, bias=True)\n",
      "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.3, inplace=False)\n",
      "        (dropout2): Dropout(p=0.3, inplace=False)\n",
      "        (activation): GELU(approximate='none')\n",
      "      )\n",
      "      (2): getFirstTokenOutput()\n",
      "      (3): Linear(in_features=128, out_features=4, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of parameters (as tensors) : 17\n",
      "Number of parameters in optimizer (as tensors) : 17\n"
     ]
    }
   ],
   "source": [
    "print(\"Rotation HEAD\")\n",
    "net = 2\n",
    "print(model.nets[net])\n",
    "\n",
    "\n",
    "len_parameters = len(list(model.nets[net].parameters()))\n",
    "\n",
    "op_params = []\n",
    "for group in model.optimizer.optimizer.param_groups:\n",
    "    for p in group[\"params\"]:\n",
    "        if f\"Net{net}\" in p.dpname:\n",
    "            op_params.append(p.dpname)\n",
    "print(f\"Number of parameters (as tensors) : {len_parameters}\")\n",
    "print(f\"Number of parameters in optimizer (as tensors) : {len(op_params)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13cff4e4",
   "metadata": {},
   "source": [
    "# Create a LearnFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4996f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint directory already exists\n"
     ]
    }
   ],
   "source": [
    "lf = LearnFrame(model,data, initialze= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71321e57-d935-4aa2-8089-ffab14a4e404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05%|\u001b[30m\u001b[90m░░░░░░░░░░\u001b[0m| 000049/100000 [0:00:43<24:41:41, 1.12it/s]"
     ]
    }
   ],
   "source": [
    "lf.train(test_freq=250, save_freq=10000, steps = steps, test_steps=gradient_accumulation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b62bbf6-a8a4-44f0-a1d4-725ff1f14b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
