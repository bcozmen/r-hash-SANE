# r-hash-SANE: Weight-Space Learning for NeRFs

> **Neural Radiance Fields meet Structured Affine Neural Embeddings.**  
> A transformer-based autoencoder for learning 3D scene representations directly in the **weight space** of Instant-NGP models.

---

## ğŸš€ Overview

This project extends the **Structured Affine Neural Embeddings (SANE)** algorithm to **Instant Neural Graphics Primitives (Instant-NGP)** â€” bringing weight-space learning into the domain of 3D scene representations.

Instead of analyzing or generating images, we analyze the **weights** of trained Instant-NGP networks to learn disentangled latent embeddings that encode **object identity** and **rotational transformations**.  

We design a transformer-based autoencoder that:
- Tokenizes both **hash grid features** and **MLP weights** from Instant-NGP.
- Learns **rotation-aware** and **object-specific** latent representations.
- Combines **reconstruction**, **contrastive**, and **rotation-prediction** objectives.

---

## ğŸ§  Key Ideas

- **Weight-space learning** instead of data-space learning:  
  Neural representations are treated as data points in parameter space.

- **Hybrid tokenization strategy**:  
  - Hash layers â†’ spatially structured 256D tokens sampled from 3D space.  
  - MLP layers â†’ grouped neuron blocks mapped to 256D tokens.

- **Rotation-aware positional encoding**:  
  Custom positional embeddings preserve 3D structure and Instant-NGP layer organization.

- **Multi-objective training**:
  ```
  L = 0.4 * L_recon + 0.05 * L_contrastive + 0.53 * L_rotation + 0.02 * ||Z||Â²
  ```

---

## ğŸ“¦ Architecture

A **transformer autoencoder** forms the core of the system:

```
Instant-NGP Weights
        â†“
Tokenization (Hash + MLP + POS)
        â†“
Encoder (Transformer)
        â†“
Latent Representation Z
        â†“
Decoder (Reconstruction)
  â”œâ”€ NT-Xent Head (Contrastive)
  â””â”€ Rotation Head (Quaternion Prediction)
```

- **Encoder/Decoder:** 4-layer transformer, 4 attention heads, 1024-d embeddings.  
- **Latent space:** 128-dim per token, split into â€œcontentâ€ and â€œrotationâ€ subspaces.  
- **[POS] tokens:** Dedicated 10 tokens for predicting rotation quaternions.  

---

## ğŸ“Š Dataset

- **342 3D objects**, each trained as **Instant-NGP** under 6 rotational viewpoints.  
- Each sample includes:
  - 16 **hash tables** (~6.5M params)
  - 6 **MLP layers** (~20K params)
- Total input sequence per model: **443 tokens Ã— 256 dims**.  

---

## âš™ï¸ Training Configuration

| Parameter | Value |
|------------|--------|
| Batch size | 32 |
| Epochs | 500 |
| Optimizer | Adam |
| Learning rate | 1e-4 |
| Weight decay | 0.01 |
| Dropout | 0.3â€“0.4 |
| Contrastive temp. | 0.1 |
| Scheduler | OneCycleLR |

---

## ğŸ§© Results Summary

| Task | Outcome |
|------|----------|
| **Reconstruction** | Converged (~0.1 loss) â€” structure preserved. |
| **Contrastive (NT-Xent)** | Successful rotation-invariant embeddings (<0.1 loss). |
| **Rotation Prediction** | Overfitting; generalization limited â€” data too sparse. |

Key insight: The model can *understand rotation-invariant structure* in NeRF weight space, but **true disentanglement of rotation vs. content** remains difficult with limited discrete rotations.

---

## ğŸ§­ Future Work

- Larger and continuous-rotation datasets.  
- Improved positional encodings (concatenative, not additive).  
- Exploring **continuous rotation interpolation** in latent space.  
- Rebalancing latent capacity between `[POS]` and content tokens.  

---

## ğŸ§‘â€ğŸ’» Authors

- **Zilong Liu** â€“ Geoinformation Science  
- **Roman Oelfken** â€“ Information Systems  
- **Berk Can Ã–zmen** â€“ Computer Science *(repo maintainer)*  
- **Can-Philipp Tura** â€“ Computer Science  

Technische UniversitÃ¤t Berlin  

---

## ğŸ“š References

- SchÃ¼rholt et al., *Towards Scalable and Versatile Weight Space Learning*, arXiv:2406.09997  
- MÃ¼ller et al., *Instant Neural Graphics Primitives*, ACM TOG, 2022  
- Mildenhall et al., *NeRF: Representing Scenes as Neural Radiance Fields*, CACM, 2021  

---

## ğŸ—ï¸ Repo Structure (expected)

```
r-hash-SANE/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ autoencoder.py
â”‚   â”‚   â”œâ”€â”€ ntx_head.py
â”‚   â”‚   â””â”€â”€ rotation_head.py
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ dataset_loader.py
â”‚   â”‚   â””â”€â”€ tokenization.py
â”‚   â””â”€â”€ train.py
â”‚
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ hyperparams.yaml
â”‚
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ reconstructions/
â”‚   â””â”€â”€ checkpoints/
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```
